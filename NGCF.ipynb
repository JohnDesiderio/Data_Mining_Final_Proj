{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loads Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off of Neural Graph Collaborative Filtering\n",
    " # Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\n",
    " # Neural Graph Collaborative Filtering. In Proceedings of the 42nd International\n",
    " # ACM SIGIR Conference on Research and Development in Information Retrieval\n",
    " # (SIGIR ’19), July 21–25, 2019, Paris, France. ACM, New York, NY, USA,\n",
    " # 10 pages. https://doi.org/10.1145/3331184.3331267\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init, LeakyReLU, Linear, Module, ModuleList, Parameter\n",
    "\n",
    "batch_size = 1024 # Set as such because it says this is the optimal number in the above paper\n",
    "\n",
    "# Gets values from csv\n",
    "input = pd.read_csv(\"ratings_small.csv\", usecols=[\"userId\", \"movieId\", \"rating\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits input into training and testing sets\n",
    "userSet = []\n",
    "spot = 1\n",
    "activeItems = []\n",
    "for a, b, c in input.values:\n",
    "    if spot == a:\n",
    "        activeItems.append(b)\n",
    "    else:\n",
    "        userSet.append(activeItems)\n",
    "        activeItems = []\n",
    "        spot += 1\n",
    "        activeItems.append(b)\n",
    "userSet.append(activeItems)\n",
    "\n",
    "training = []\n",
    "testing = []\n",
    "for i in range(len(userSet)):\n",
    "    length = int(len(userSet[i])*0.7)\n",
    "    for a in userSet[i][:length]:\n",
    "        training.append([i+1, a])\n",
    "    for a in userSet[i][length:]:\n",
    "        testing.append([i+1, a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets lists of unique values\n",
    "userIDList = input['userId'].unique()\n",
    "movieIDList = input['movieId'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_item_matrix(matrix):\n",
    "    output = pd.DataFrame(input, columns=movieIDList, index=userIDList)\n",
    "    for a, b in matrix:\n",
    "        output.at[a, b] = 1\n",
    "    output.fillna(value = 0, inplace = True)\n",
    "    output = torch.from_numpy(output.values)\n",
    "    return output\n",
    "\n",
    "# Builds the user item interaction matrix (training)\n",
    "userItemIteractionMatrix = get_user_item_matrix(training)\n",
    "# Builds the user item interaction matrix (testing)\n",
    "testingUIMatrix = get_user_item_matrix(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds the positive list\n",
    "def build_positive_list(matrix):\n",
    "    positiveList = []\n",
    "    spot = 1\n",
    "    temp = []\n",
    "    for a, b in matrix:\n",
    "        b = int(b)\n",
    "        if a == spot:\n",
    "            temp.append(b)\n",
    "        else:\n",
    "            spot += 1\n",
    "            positiveList.append(temp)\n",
    "            temp = [b]\n",
    "    positiveList.append(temp)\n",
    "    return positiveList\n",
    "\n",
    "positiveList = build_positive_list(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creates Adjacency Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jgill\\AppData\\Local\\Temp\\ipykernel_15280\\2393360537.py:12: RuntimeWarning: divide by zero encountered in power\n",
      "  d = np.power(rowsum, -1).flatten()\n"
     ]
    }
   ],
   "source": [
    "numberOfUsers = len(userIDList)\n",
    "numberOfItems = len(movieIDList)\n",
    "\n",
    "AdjanceyMatrix = np.zeros((numberOfUsers + numberOfItems, numberOfItems + numberOfUsers))\n",
    "AdjanceyMatrix[:numberOfUsers, numberOfUsers:] = userItemIteractionMatrix.tolist()\n",
    "AdjanceyMatrix[numberOfUsers:, :numberOfUsers] = userItemIteractionMatrix.T.tolist()\n",
    "\n",
    "rowsum = np.array(AdjanceyMatrix.sum(1))\n",
    "\n",
    "# Tecnique taken from the paper's implementaion instead of the paper itself\n",
    " # https://github.com/xiangwang1223/neural_graph_collaborative_filtering\n",
    "d = np.power(rowsum, -1).flatten()\n",
    "d[np.isinf(d)] = 0.\n",
    "d_mat = np.zeros((len(d), len(d)))\n",
    "for i in range(len(d)):\n",
    "    d_mat[i][i] = d[i]\n",
    "NormalizedAdjacencyMatrix = d_mat.dot(AdjanceyMatrix)\n",
    "stuff = torch.from_numpy(AdjanceyMatrix).to_sparse()\n",
    "Final = NormalizedAdjacencyMatrix + np.eye(NormalizedAdjacencyMatrix.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def positive(u):\n",
    "    movie = random.randint(0, len(positiveList[u-1])-1)\n",
    "    for i in range(len(movieIDList)):\n",
    "        if movieIDList[i] == positiveList[u-1][movie]:\n",
    "            return i\n",
    "\n",
    "def negative(u):\n",
    "    while True:\n",
    "        movie = random.randint(0, numberOfItems-1)\n",
    "        movie = movieIDList[movie]\n",
    "        if movie not in positiveList[u-1]:\n",
    "            for i in range(len(movieIDList)):\n",
    "                if movieIDList[i] == movie:\n",
    "                    return i\n",
    "\n",
    "def get_a_sample():\n",
    "    selectedUsers = np.random.choice(userIDList, size=batch_size)\n",
    "    pos_sample, neg_sample = [], []\n",
    "    for u in selectedUsers:\n",
    "        pos_sample.append(positive(u))\n",
    "        neg_sample.append(negative(u))\n",
    "    return selectedUsers, np.asarray(pos_sample), np.asarray(neg_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfLayers = 3\n",
    "\n",
    "class NGCF(Module):\n",
    "  def __init__(self, numberOfUsers, numberOfItems, embed_size, adjacencyMatrix):\n",
    "    super().__init__()\n",
    "    self.numberOfUsers = numberOfUsers\n",
    "    self.numberOfItems = numberOfItems\n",
    "    self.embed_size = embed_size\n",
    "    self.adj_matrix = adjacencyMatrix\n",
    "\n",
    "    # This portion is adapted from https://medium.com/@meuleman.mathias/reproducing-neural-graph-collaborative-filtering-a8982c7d3df6 since I still don't understand how Parameter works  \n",
    "    self.user_embeddings = Parameter(torch.rand(numberOfUsers, embed_size))\n",
    "    self.item_embeddings = Parameter(torch.rand(numberOfItems, embed_size))\n",
    "    self.user_embeddings_final = Parameter(torch.zeros((numberOfUsers, embed_size * (numberOfLayers + 1))))\n",
    "    self.item_embeddings_final = Parameter(torch.zeros((numberOfItems, embed_size * (numberOfLayers + 1))))\n",
    "    self.W1 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, numberOfLayers)])\n",
    "    self.W2 = ModuleList([Linear(self.embed_size, self.embed_size) for _ in range(0, numberOfLayers)])\n",
    "    self.reLU = LeakyReLU()\n",
    "    self.init_weights()\n",
    "\n",
    "  def init_weights(self):\n",
    "    for name, parameter in self.named_parameters():\n",
    "      if ('bias' not in name):\n",
    "        init.xavier_uniform_(parameter)\n",
    "\n",
    "  def forward(self, u, i, j):\n",
    "    embeddings = torch.cat((self.user_embeddings, self.item_embeddings))\n",
    "    final_embeddings = [embeddings]\n",
    "\n",
    "    # Does the messaging\n",
    "    for l in range(numberOfLayers):\n",
    "      t1_embeddings = torch.sparse.mm(torch.from_numpy(self.adj_matrix.astype(np.float32)).to_sparse(), embeddings)\n",
    "      t1 = self.W1[l](t1_embeddings)\n",
    "      t2_embeddings = embeddings.mul(t1_embeddings)\n",
    "      t2 = self.W2[l](t2_embeddings)\n",
    "      embeddings = self.reLU(t1 + t2)\n",
    "      normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "      final_embeddings.append(normalized_embeddings)\n",
    "\n",
    "    final_embeddings = torch.cat(final_embeddings, 1)\n",
    "    final_u_embeddings, final_i_embeddings = final_embeddings.split((self.numberOfUsers, self.numberOfItems), 0)\n",
    "    self.user_embeddings_final = Parameter(final_u_embeddings)\n",
    "    self.item_embeddings_final = Parameter(final_i_embeddings)\n",
    "\n",
    "    return self.compute_loss(final_u_embeddings[u-1], final_i_embeddings[i], final_i_embeddings[j])\n",
    "\n",
    "  def compute_loss(self, userE, posE, negE):\n",
    "    return -(torch.log(torch.sigmoid(torch.mul(userE, posE).sum(dim=1) - torch.mul(userE, negE).sum(dim=1)))).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " Loss = 3.6117654144763947\n",
      "Epoch 2\n",
      " Loss = 2.2050976306200027\n",
      "Epoch 3\n",
      " Loss = 1.8743716776371002\n",
      "Epoch 4\n",
      " Loss = 1.6849270910024643\n",
      "Epoch 5\n",
      " Loss = 1.5634060502052307\n",
      "Epoch 6\n",
      " Loss = 1.5372567921876907\n",
      "Epoch 7\n",
      " Loss = 1.406380757689476\n",
      "Epoch 8\n",
      " Loss = 1.3347729742527008\n",
      "Epoch 9\n",
      " Loss = 1.2633979618549347\n",
      "Epoch 10\n",
      " Loss = 1.1762020513415337\n"
     ]
    }
   ],
   "source": [
    "model = NGCF(numberOfUsers=numberOfUsers, numberOfItems=numberOfItems, embed_size=64, adjacencyMatrix=Final)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "epochs = 10\n",
    "\n",
    "model.train()\n",
    "n_batch = numberOfItems // batch_size + 1\n",
    "\n",
    "def train(model, t):\n",
    "  print('Epoch ' + str(t+1))\n",
    "  total_loss = 0\n",
    "  for _ in range(n_batch):\n",
    "    user, pos, neg = get_a_sample()\n",
    "    optimizer.zero_grad()\n",
    "    loss = model(torch.from_numpy(user).long(), torch.LongTensor(pos), torch.LongTensor(neg))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "  print(' Loss = ' + str(total_loss))\n",
    "\n",
    "for t in range(epochs):\n",
    "  train(model, t)\n",
    "  model.train()\n",
    "  model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Precision:\t0.14456035767511177\n",
      "Recall:\t\t0.011635772263848695\n"
     ]
    }
   ],
   "source": [
    "# Gets embedings from model\n",
    "user_embeddings = model.user_embeddings_final.detach()\n",
    "item_embeddings = model.item_embeddings_final.detach()\n",
    "\n",
    "# Calculates precision and recall\n",
    "recall_parts, ndcg_parts = [], []\n",
    "\n",
    "non_train_items = (1 - user_embeddings).float()\n",
    "predictions = torch.mm(user_embeddings, item_embeddings.t())\n",
    "predictions = predictions * (1-userItemIteractionMatrix)\n",
    "\n",
    "output = np.zeros((numberOfUsers, numberOfItems))\n",
    "reversedMatrix = 1-userItemIteractionMatrix\n",
    "top10 = torch.topk(predictions * reversedMatrix, k=10).indices\n",
    "for u in range(len(top10)):\n",
    "  for i in top10[u]:\n",
    "    output[u][i] = 1\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "\n",
    "truePositives = []\n",
    "falsePositive = []\n",
    "falseNegatives = []\n",
    "for user in range(len(output)):\n",
    "  TPtemp = 0\n",
    "  FPtemp = 0\n",
    "  FNtemp = 0\n",
    "  for i in range(len(output[user])):\n",
    "    if output[user][i] == 1 and testingUIMatrix[user][i] == 1:\n",
    "      TPtemp += 1\n",
    "    elif testingUIMatrix[user][i] == 1 and output[user][i] == 0: #false negative\n",
    "      FNtemp += 1\n",
    "    elif testingUIMatrix[user][i] == 1 and output[user][i] == 0: #false positive\n",
    "      FPtemp += 1\n",
    "  truePositives.append(TPtemp)\n",
    "  falsePositive.append(FPtemp)\n",
    "  falseNegatives.append(FNtemp)\n",
    "\n",
    "precision = 0\n",
    "recall = 0\n",
    "TPs = 0\n",
    "for i in range(len(truePositives)):\n",
    "  if (truePositives[i] + falsePositive[i]) != 0:\n",
    "    precision += truePositives[i]/(truePositives[i] + falsePositive[i])\n",
    "  if (truePositives[i] + falseNegatives[i]) != 0:\n",
    "    recall += truePositives[i]/(truePositives[i] + falseNegatives[i])\n",
    "  TPs += truePositives[i]\n",
    "  \n",
    "precision = precision/len(truePositives)\n",
    "recall = recall/len(truePositives)\n",
    "falseNegatives = TPs/len(falseNegatives)\n",
    "\n",
    "print('Precision:\\t' + str(precision))\n",
    "print('Recall:\\t\\t' + str(recall))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3af5524c48f43e24cd2e6132989d065113de7323b7f93513999bc401a001bbaf"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
